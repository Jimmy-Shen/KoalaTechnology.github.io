---
layout: post
title:  "关于Streaming101"
categories: flink
tags:  大数据 flink streaming  
author: 神奇的考拉

---

* content
{:toc}


### 一.术语

#### **什么是streaming**

现在关于streaming有很多不同的解释和说明，因而在深入讨论话题之前，先确定：什么是streaming;接下来搞清楚streaming系统到底能解决什么问题.在本章节我们会得到关于streaming的“精确定义”。

经常我们会听到“unbounded data processing(无界数据处理)”，“approximate results(近似结果)”等诸如此类的术语，并被认为是“streaming”，而实际上是用streaming来解决此类问题，同样batch也能解决。甚至在某些情况下，人们会狭隘地认为streaming只是拥有某些被称作streaming的特性，比如“approximate results(近似结果)”，“ speculative results(推测结果)”等，一个设计合理的steaming应该像batch engine(批处理引擎)一样产生正确的(correct)、一致的(consistent)、可重复(repeatable)的结果，故而对于streaming的定义：一种能够处理无界数据的数据处理引擎。

> 从定义的准确性和完整性来说，关于streaming的定义包括了真正意义的streaming和micro-batch微批处理。

下面会罗列一些经常和streaming联系在一起的几个术语：

**1.无界数据(unbounded data)**：一种持续产生并且无界的数据集，常常会被认为是streaming数据。但是，使用streaming和batch这两个术语来描述数据集是有问题的，因为它们只是表示处理数据集的某种执行引擎类型。而这两类数据集的不同点关键在于：是否有界(finitness)？所以在说无界的”streaming”数据称为unbounded data，将有限的”batch”数据称为bounded data。

**2.无界数据处理unbounded data processing**: 一种用来处理无界数据的处理模式/方式；常应用于unbounded data。为了区分使用streaming来描述这种类型的数据处理还是streaming流式计算引擎，故而使用unbounded data processing。(在实际的batch处理引擎中，可以通过重复执行也是能够完成unbounded data处理，反之一个设计良好的streaming system也是能够处理bounded data)

**3.低延迟low-latency、近似结果approximate results、推测结果speculative results**：这种类型的计算常常和streaming引擎关联在一起的。由于batch处理系统通常不被设计用来解决低延迟low-latency、近似结果approximate results的，这是大家的固有印象。而事实上，batch批处理引擎也是可以产生approximate results近似结果的，因此，这些术语可以很好的描述它们是什么(**低延迟low-latency、近似结果approximate results、推测结果speculative results**),而不是使用streaming流式处理引擎来代表他们。

到这里，当我们使用streaming术语时，可直接认为是：**一种用来处理无界数据集的执行引擎，而没有其他**。而当提及其他术语时，直接说：无界数据unbounded data、无界数据处理unbounded data processiong、或者低延迟low-latency、近似结果approximate results、推测结果speculative results。这样可以便于我们能够很好理解streaming，以及其他关联的术语。

#### **streaming system的局限性**

一直以来，streaming系统被狭隘地认为是提供低延迟low-latency、近似结果approximate results，与提供最终正确结果的batch系统，构成Lambda架构。接下来，会讨论一些streaming系统能做的和不能做的，以及最关键的一个设计良好的streaming system流式系统能做什么。

Lambda架构(*见引用2*)是由storm的创始人：**Nathna Marz**提出来的。Lambda系统基本理念就是同时运行一个streaming系统和batch系统：steaming系统提供低延迟low-latency、近似结果approximate results(用于使用的近似算法或系统本身就不提供准确性保证),过段时间后batch系统提供准确的结果输出。Lambda架构提供了一种短期的解决方案，在当时streaming系统的正确性不够令人满意，batch系统也没有预期那么好的情况下，使得不少项目采用。但Lambda架构本身存在的问题也尤为突出：维护成本高，需要同时维护两个独立的系统，最终还需要合并它们的结果。

在Jap Kreps的文章**Questioning the Lambda Architecture**(*见引用3*)中提出了对双引擎模式必要性的质疑，并给出了强有力的依据。Kreps使用可重放的消息系统比如kafka来解决可重复性repeatability问题，并提出了一个新的Kappa架构，利用设计良好的系统采用pipeline单系统来运行。

不过设计合理的streaming系统能够提供比batch系统更多的功能，除去资源利用率方面的考虑，是可以完全不需要batch系统。Flink(*见引用4*)就是采用了这种想法构建完全的streaming系统，并支持batch模式。

随着streaming系统的逐渐成熟，它会提供一种完整、健壮的unbounded data无界数据处理框架，进而Lambda架构也会逐渐消失。不过streaming系统彻底取代batch系统，需要完成如下两点：

1.**正确性correctness** - 使得streaming能和batch等同

关于正确性correctness从本质上来说，其归结于**一致性consistent**存储的问题。straming系统需要一种类似给状态state创建持久化检查点checkpoint的方法(*见引用5***“Why local state is a fundamental primitive in stream processing”**)，并且它必须设计的足够好，即使在机器挂掉的情况下也能保证一致性consistent。需要强调的一点：streaming系统达到甚至超越batch系统，是需要达成强一致性的，完成exactly-once语义的，这样才能保证结果的一致性(除非你的streaming系统对结果的正确性毫不在意)。

2.**时间推理reasoning about time** - 是streaming系统超越batch系统的关键

在实际的业务过程中，会面对处理无界、无序的数据时，具体的事件采用的时间推理就变得不可或缺，并且现有的batch系统(也包括大多数流式系统)都缺乏必要合适时间推理逻辑，来解决无界、无序的数据问题。

故而我们需要理清Time相关的一些概念，接下来深入分析在batch系统和streaming系统中关于bounded和unbounded  data processing所涉及的Time：

**Event Time vs Processing Time**

- Event time：事件真正发生的时间
- Processing time：事件进入系统被处理的时间

### **二.关于Time**

在实际的应用场景中是依赖Event time还是Processing time，对于处理者来说是不小的挑战。相对来说基于Processing time的处理是比Event time的简单了许多(后面会详细来说明)。

理想情况下，Event time和Process time应该是一直相同的，也就是说时间发生时即被处理。但实际情况却非如此，Event time和Processing time之间是存在gap，而如下的因素经常会影响两者间的gap：

- 共享资源的限制，比如网络拥塞、网络出现分区或非独占环境中的共享CPU
- 软件因素，比如分布式系统逻辑、冲突等
- 数据本身的特征，包括key的分布，吞吐量差异，数据无序/乱序等

那么现实中的Event time和Process time的关系会如下图所示：

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200410131956036.png" alt="image-20200410131956036" style="zoom:50%;" />

图示说明：黑色虚线代表理想状态下，即Event time和Process time完全相同；红色实线表示实际情况。图中理想状态黑色虚线和实际情况红色实线之间的水平距离的skew就是event time和processing time实际的差异。

另外，由于event time和processing time之间的映射关系并不是固定不变的，这意味着在实际应用中如果我们比较关于event time事件真正发生的时间，那就很难使用processing time来“取而代之”。为了很好的处理unbounded data时，对应的系统会将数据采用window time来进行划分，这样就将数据转化为一个个bounded的数据小块来进行处理。

1. 如果关心数据的正确性，那就意味着我们需要按照event事件实际发生的时间点来分析数据，而不能使用Processing time来定义一个个 window time的临界线：因为event time和processing time之间会存在skew，就会导致一些数据被划分到错误的基于processing time窗口中，进而影响到最终数据的正确性；
2. 数据完整性呢？如果实际应用中我们采用event time来做window的临时分界线，来面对unbounded data的乱序/无序和变化时间差的问题：比如在给定Event time：X，是否能够确保Event time：X之前的数据都已经被接收了？在实际中很多数据处理系统都必须依赖某种完整性，否则数据的完整性就很难保证。那么在面对undbounded data时这个挑战将会更加严重；

在flink部分我们还会深入探讨time。接下来我们将会了解下batch和streaming系统常用的数据处理模式(data processing pattern).



**数据处理模式(Data Processing Pattern)**

###### 1.有界数据 bounded data

针对有界数据bounded data的处理方法是比较简单，通过数据处理引擎直接对这些无结构化或结构化的数据进行处理转换成新的结构化数据集，更符合实际业务需求的数据。比如下图通过mapreduce来进行数据处理

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200410135504865.png" alt="image-20200410135504865" style="zoom:50%;" />

上图是一个典型的批引擎处理有界数据，将左边有限的无结构化或结构化的数据通过数据处理引擎的处理产生出右边新的结构化且价值更大的数据。不过整体来说这种有界数据处理是比较简单的，可能这种计算模式存在很多变种。

针对这种无界数据unbounded data我们使用batch引擎、streaming引擎、micro-batch引擎处理，接下来我们来看看其各自的处理模式。

###### 2.无界数据 unbounded data---batch

**固定窗口(fixed window)**

通常是将输入的数据划分成固定大小的窗口，然后重复利用batch引擎来执行，将一个个固定大小的窗口当成独立的、有界的数据来处理，比如实际场景中各类日志，将event写入到目录和文件的组织结构中，并将按照event time划分的window编码到目录文件组织结构中，那么event就能shuffle到对应一个个独立的、固定大小的window中。整个过程看起来是比较简单的，但是实际处理系统还需要考虑数据完整性问题：在真实业务会存在一些事件因网络问题导致延迟到达，那么针对这部分数据该如何处理？这里就需要采用一些特别的处理方法：延迟处理等到所有事件全部都已达到或者针对有延迟事件到达的窗口的整个数据集进行重新处理等等。

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200410144443005.png" alt="image-20200410144443005" style="zoom:50%;" />

**会话窗口(session window)**

相比于固定窗口fixed window来说，会话窗口session window会将unbounded data拆分更细致，这样就会创建出更为复杂的window，若是通过batch引擎来处理，会遇到更多的问题。一般会话session是按照活跃周期划分的，计算的时候就会出现一个session跨多个不同的数据分片中。如下图：红色部分代表同一个session跨不同的分片。

![enter image description here](http://coredumper.cn/wordpress/wp-content/uploads/2018/03/figure4.jpg)

可以通过增加分片的大小能把一个session的完整数据都在一个分片上，减少了跨多个分片的概率，但同时增加了数据延迟的概率；甚至可以通过针对不同用户跨多个分片的情况采用额外的处理逻辑，这样尽量了降低了数据处理的延迟，但处理逻辑复杂化了。

###### 2.无界数据 unbounded data---streaming

通过上面的介绍可以看到使用batch引擎来处理unbounded data的有利面，也有不利处。和batch引擎处理unbounded data不同，streaming引擎是专门用来处理unbounded data，在现实世界中unbounded data本身数据特征：

- 本身数据是无界的；

- 按照event time来分析数据，就需要面对shuffle处理；进而出现无序/乱序的问题；
- event time skew是不确定的，就很难通过使用X+Y时间点处能看到大部分event time为X的数据；

针对具有如上特征的数据常用的处理方法如下：

- 时间无关：不关注time；
- 近似法：允许数据和实际存在一定的偏差；
- 基于processing time的window来处理：按照event被处理时的time来处理，比较适合对event实际发生时间需求不那么强的场景，不需要担心延迟；
- 基于event time的window来处理：按照event实际发生time来处理，针对延迟/乱序需要特殊处理；

**方法一：时间无关处理**

该方法常用于对不关系time的场景中，所有的逻辑都是有数据来驱动的，所有的事情都是由到达的数据所决定的，并不需要streaming引擎除了数据传递的功能外其他任何特殊功能。目前来说batch引擎和streaming引擎都支持这种与时间无关的应用场景，常见的使用：

a、过滤filter

比如针对日志过滤出某个特定类型或主题的内容，只需要查看到达的日志是否属于某个类型或主题，因为在任何时刻都只依赖单条日志，即使日志数据是无界unbounded data、无序的甚至event time skew是否确定都不会影响其结果。如下图：过滤

![enter image description here](http://coredumper.cn/wordpress/wp-content/uploads/2018/03/figure5.jpg)

b、哈希连接inner-join

当进行两个无界数据源进行inner-join时，只需要关心最终的join结果，处理逻辑不需要考虑时间的因素：将其中一个数据源的一个值临时缓存起来等待预期匹配的另一个数据源的值到来，再将inner-join结果发送即可。

> outer-join是不可以的，它产生数据完整性的问题：拼接的一边数据，另一个数据源数据是否到达了是无法得知的;此时可以引入timeout机制，与此同时也引入了时间因素。

**方法二：近似法处理**

近似法是对无界数据unbounded data通过一个复杂算法的处理进而得到一个相似值。近似法相对来说是以相对较低的开销来处理无界数据unbounded data来设计的，但是其本身算法通常比较复杂，外加上结果近似特性也限制了其应用。不过需要注意的时间应用在相似算法上的设计都会有一些时间因素，故而processing time时这些算法的不错选择，随着数据元素的到达来处理，并提供可控的错误边界。

![enter image description here](http://coredumper.cn/wordpress/wp-content/uploads/2018/03/figure7.jpg)

在开始介绍剩下两种方法前，需要重点讲述下“***时间窗口(time window)***”

时间窗口(time window)就是将数据源(unbounded或bounded)沿着对应的时间线分成有限的数据块进行处理，常见的窗口模式如下：

![enter image description here](http://coredumper.cn/wordpress/wp-content/uploads/2018/03/figure8.jpg)

- 固定窗口fixed window

  通过将window按照固定时间长度的段来划分； 固定窗口的段会覆盖整个数据集

- 滚动窗口sliding window

  算是固定窗口的一种广义形式，它是通过一个固定长度和固定间隔来定义的。当window's  interval < window's length,则窗口间存在重叠；若window‘s interval = window‘s length，则就是固定窗口；当window's interval > window's length,则为取样窗口，只会查看部分数据。

- 会话窗口session window

  会话窗口session window应该属于动态窗口的一种；通过超过某个时长的非活跃间隙切分的事件集组成的window。这样就可以将一系列时间相关的事件划分到一起，该窗口常用于用户行为分析。在实际中由于不同的数据子集(比如不同的用户)其会话也是不同的，会话的长度无法提前定义的，主要是依赖实际的数据。

上面也罗列三种不同类型的window，也简单介绍其定义。不过还有一点需要注意：window会涉及是否对齐align.比如在固定窗口应用中，若是对应的window覆盖到了所有的数据，可视其为对齐窗口(aligned-window);不过在某些特殊情况下，需要为不同的数据子集移动window，这样window的负载会随着时间的推移越来越均匀，此时可视为unaligned-window。而滑动窗口通常是对齐窗口aligned-window应用于所有的数据集上；但会话窗口因其本身的特性：数据子集需要移动window，属于未对齐窗口unaligned-window。

**方法三：基于processing time窗口处理**

![image-20200416171816273](/Users/wangmm02/Library/Application Support/typora-user-images/image-20200416171816273.png)

当采用processing time来创建window，系统会将到来的数据buffered到window直至超过***指定的processing time***，例如，5mins固定window，系统会按照processing time将5mins数据缓存，接着将这5mins内收到的数据封装进行一个window并send给下游进行处理。不过使用processing time来进行window划分还会带来不错的好处：

- 简单，只需要buffered到来的数据，等待window触发关闭时，将数据发送给下游即可，不需要根据时间shuffle数据
- window的完整性容易判断，因为基于processing time划分的window完全不需要关心晚到的数据如何处理，系统本身完全知道某个window的输入数据是否全部到达
- 适用于根据观测来推测数据源相关信息的场景，比如日常中很多监控场景，根据统计当前service每秒请求数来判断当前service是否健康等

但有一点若是需要processing time窗口反映出event真正发生的时间，这种基于processing time的window就很难满足需求了，是需要event按照其真正发生的时间有序发送到window。在实际业务中按照event time有序的数据是几乎不存在的，会面临网络延迟、时钟不同步等情况，就很难达到不乱序、不延迟的数据“流入”window。当整个系统都处于健康运行的状态下，event-time skew看起来很低甚至可以忽略，但并不意味着能够一直保持如此，出现任何的网络问题都会造成某一部分输入数据的event-time skew变大，而由于基于processing time划分的window是不关心这些的，就无法表示数据真正产生的情况，新旧数据就会交替在一起。**processing time仅表示事件到达的时间。**

**方法四：基于event time处理**

当需要反映事件真实发生的时间时需要使用基于event time划分的时间窗口。如下图基于event time将unbouned data划分为1hour的固定时间窗口

![image-20200416172441921](/Users/wangmm02/Library/Application Support/typora-user-images/image-20200416172441921.png)

如上图所示，所有的event都按照其event-time划分到其真正产生的事件时间窗口中，图中的两个白色箭头显示了将延迟的event能划分到其真正事件时间window里面，这也是不同于基于processing time划分的window，换句话说就是数据所在的processing time和event time的window是不一定一致的。能够提供event time的正确性是event-time window的一大优势。同时基于event time来创建动态大小的window能够满足session的需求，如下图所示：

![image-20200416181508074](/Users/wangmm02/Library/Application Support/typora-user-images/image-20200416181508074.png)

基于event-time来划分session时间窗口，会通过shuffle操作来将数据放入正确的event-time的时间窗口里面。但由于event-time窗口需要一直buffered输入的数据，这样就会导致window存在的时间比窗口本身的长度更长些，会带来比较显著的问题：

- 缓存buffered：因需要缓存更多的数据，来确保其能够按照event-time发送到其真正正确的window，延长了window的生命周期。可能需要提供一种state机制来完整增量处理或评估window已完成的方式，并对后续的延迟的数据提供保障，比如watermark/延迟机制等。
- 完整性：由于需要一直buffer数据直至当前的event-time时间窗口全部到齐，才能处理该window的数据。但这个时间对我们来说可能是不确定的。需要我们提供一种机制来给出一个window完整性的评估，比如watermark；甚至还需要一种能够重新处理window中的数据不断的修正计算结果。

### 三.关于一致性(consistency)

目前已有较多streaming流计算系统被广泛应用，诸如 Apache Storm，Apache Flink, Heron, Apache Kafka (Kafka Streams) 和 Apache Spark (Spark Streaming)。但流计算系统有个被广泛讨论的特性是『exactly-once』语义，并且很多系统宣称已经支持了这一特性。但是，到底什么是『exactly-once』，怎么样才算是实现了『exactly-once』，人们存在很多误解和歧义，接下来我们会做下分析。常说的一次性的语义内容如下：

- 最多一次性 At-most-once
- 至少一次性 At-least-once
- 精确一次性 Exactly-once

streaming流处理可以简单的描述为是对unbounded data或events的连续处理，甚至也可以将steraming处理程序描述为有向无环图(DAG)，如下图

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200420095854401.png" alt="image-20200420095854401" style="zoom:50%;" />

在这样的图中，每条边(edge)代表数据或事件流，每个顶点(vertex)代表运算(operator)，会使用程序中定义的逻辑来处理来自其上游相邻边的数据或事件。不过在图中定义的所有的顶点(vertex)中有两类顶点(vertex)是比较特殊的：source和sink；***source***代表读取外部数据或事件到streaming流计算系统中，而***sink***是用于收集程序生产的结果。

streaming流处理引擎通过会提供可靠性模式或语义来供用户选择使用，来为整个应用程序中的数据处理提供何种保证。因面临网络、机器等可能导致数据丢失的故障，这些保证就显得有意义了。

### 最多一次:  At-most-once

在该模式或语义下，只是『尽力而为』来保证数据或事件最多由application中所有的算子处理一次，也就意味着数据在被streaming流应用程序处理前发生了数据丢失，则不会进行任何重试(retry)机制或流回放。

![image-20200420101752046](/Users/wangmm02/Library/Application Support/typora-user-images/image-20200420101752046.png)

### 至少一次性：At-least-once

在此模式或语义，application中所有的算子能保证数据或事件至少被处理一次。换而言之，若是数据或事件在streaming流应用程序处理前已出现数据丢失，则会进行source重放或重新传输事件。但再流重放或重新传输事件过程中，会存在事件被重传，一个事件就会被处理多次。

![image-20200420102216892](/Users/wangmm02/Library/Application Support/typora-user-images/image-20200420102216892.png)

### 精确一次性：Exactly-once

即使是在各种故障的情况下，streaming流应用程序中的所有算子都保证事件只会被『精确一次』的处理。就是该一次性模式或语义所提供的保证。

> Exactly-once也会被说是完全一次性、恰好一次性

常见比较流行的『精确一次』实现机制：

- 分布式快照或状态检查点
- 至少一次性事件传递和去重保证

##### 关于Chandy-Lamport分布式快照算法

在chandy-lamport的实现中，通过对streaming流应用程序中每个算子的所有状态定期做checkpoint。若是在某个环节发生了失败failed，那每个算子的所有状态都回滚到最新的全局一致的checkpoint点。在回滚期间，会导致所有的处理暂停。源source也会重置到最近的一次checkpoint相对应的正确偏移量offset位置，这样就使得streaming流应用程序基本上回到最近一次的一致状态，接着呢，程序就可以开始从该状态重新执行。

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200420103438111.png" alt="image-20200420103438111" style="zoom:50%;" />

如图所示，streaming流应用程序在T1时间处于正常工作，同时也做了checkpoint。但是，在时间T2处，算子并未完成对输入数据的真正处理，此时面临情况：S=4的状态值已被持久化，而新输入数据计算的状态值S=12存在内存中，这就导致了差异。在时间T3处，处理程序首先将状态state回滚到S=4并将流中的每个状态state都回放到T1处的状态，并处理每个数据，在此过程中，有些算子operator可能对有些数据进行了多次处理，但其所有的状态值都是相同的。

#### 关于至少一次性事件传递和去重保证算法

通过对每个算子上实现至少一次性事件传递和去重保证，当进行重放失败事件时在事件进入算子的用户定义逻辑前，会进行一次处理并移除每个算子的重复事件，此机制需为每个算子维护一个事务日志，并跟踪其已处理的事件。

<img src="/Users/wangmm02/Library/Application Support/typora-user-images/image-20200420105658422.png" alt="image-20200420105658422" style="zoom:67%;" />

##### **分布式快照/状态检查点与至少一次事件传递和重复数据删除的比较**

首先从语义来看，无论是分布式快照还是至少一次事件传递及去重保证机制都提供了相同的保证；

其次两者实现的差异也就会导致存在显著性能差异：

- 分布式快照/状态检查点 相对性能开销是最小的

​       往streaming流应用程序中所有的算子一起发送事件，同时状态检查点是在后台异步执行，此非侵入式的机制在运行时所需的额外资源较少。不过若是故障频繁的发生，因streaming流处理引擎需要暂停应用程序并回滚所有的算子状态，会带来性能的影响。流式应用程序越大，发生故障的概率相对高些，其性能所受到的影响也就越大。

- 至少一次事件传递及去重保证机制，需要更多的资源来存储事务日志

​        使用此机制需要能够跟踪每个算子已完成处理的每个元组，以便完成执行重复数据的去重，这就意味着需要跟踪大量的数据。尤其是streaming流应用程序比较大或有很多应用程序都处于运行的情况下，执行去重的每个算子的每个事件都会产生性能开销。不过该机制下应用程序性能不大可能受应用程序大小所影响的。不同于机制1当任何算子发送故障时需要发生全局暂停并进行状态回滚，机制2发生故障的算子更加局部性，只需将尚未完全处理的事件从上游重放或重传，这样性能影响和streaming流应用程序发生故障的位置是隔离的，对其他算子的性能影响也是微乎其微的。

最后两种机制的优缺点：

1. **分布式快照/状态检查点优缺点**
   - 优点： a、较小的资源开销和较低的性能开销
   - 缺点： a、对性能影响较大  b、streaming流应用程序拓扑越大，其对性能的潜在影响就越大

2. **至少一次事件传递和去重机制优缺点**
   - 优点  ：a、故障对性能影响是局部的 b、故障影响不一定随着拓扑的大小而增加
   - 缺点  ：a、需要更多的存储和基础设施支撑 b、每个算子的每个事件都有性能开销

如上两种机制都可以归结为至少一次的幂等性处理，当发生故障时，事件会被重放/重传并通过状态回滚或事件去重来保证算子处理数据的幂等性。

### **Exactly-once是否做到真正的『精确一次』?**

在实际应用中我们可能会认为『精确一次』在流中的每个事件只被处理一次来提供事件处理的保证。实际上很难有处理引擎能够保证事件正好只处理一次。很难保证每个算子中的用户逻辑对每个事件只执行一次，用户代码被部分执行的可能性会一直存在的。『精确一次』这个术语在描述正好处理一次时会让人产生误导。

在用户自定义的逻辑代码执行过程中，可能会存在某些不可预估的故障。那streaming流式计算引擎很难确定执行用户自定义处理逻辑的时间点，也就不能保证用户自定义逻辑只执行一次，若是在用户自定义的逻辑实现了外部操作(比如写数据库/消息队列/缓存等)也不能保证精确执行一次的，此类操作仍需要幂等性的保证来执行的。

在实际应用中，当streaming流式计算引擎声明『精确一次』处理语义时，又代表什么呢？其实这些streaming流式计算引擎只是保证了引擎管理的状态更新只提交一次到后段存储中持久化。对于前面提到的两种机制都是使用持久化的后段存储来保存每个算子operator的状态并自动向其提交更新。对于分布式快照/状态检查点的机制来说，后段存储用于保存流应用程序的全局一致性状态检查点(每个算子的检查点状态)。而至少一次事件传递和去重机制使用持久化后端存储每个算子的状态以及每个算子涉及的事务日志，通过事务日志来跟踪其算子已完成处理的所有事件。

『精确一次』更多的体现在持久化后端的状态state提交，并应用更新的过程。比如计算状态的更新/变更，在输入的事件上执行用户自定义的逻辑，若是发生故障，那么对该事件的处理可能会发生多次，但对该处理的效果只会在持久化后段状态存储中反映一次，只产生一次有效操作效果。

### 引用

1.[the-world-beyond-batch-streaming-101](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/)

2.[Lambda Architecture](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html)

3.[Questioning the Lambda Architecture](http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html)

4.[Flink](http://flink.apache.org/)

5.[Why local state is a fundamental primitive in stream processing](http://radar.oreilly.com/2014/07/why-local-state-is-a-fundamental-primitive-in-stream-processing.html)

6.[Millwheel](http://static.googleusercontent.com/media/research.google.com/en/pubs/archive/41378.pdf)

7.[Spark Streaming](https://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf)

8.Chandy, K. Mani and Leslie Lamport.Distributed snapshots: Determining global states of distributed systems. ACMTransactions on Computer Systems (TOCS) 3.1 (1985): 63-75

9.Akidau, Tyler, et al. MillWheel:Fault-tolerant stream processing at internet scale. Proceedings of the VLDBEndowment 6.11 (2013): 1033-1044

